####
## Docker Compose for Agentic Data Quality Triage
## Author: Mario Caesar // caesarmario87@gmail.com
####

# infra/docker-compose.yml
name: agentic-dq-triage

x-airflow-common: &airflow-common
  image: apache/airflow:${AIRFLOW_VERSION}
  env_file:
    - ./.env
  environment:
    AIRFLOW__CORE__EXECUTOR: ${AIRFLOW__CORE__EXECUTOR}
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: ${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN}
    AIRFLOW__CELERY__BROKER_URL: ${AIRFLOW__CELERY__BROKER_URL}
    AIRFLOW__CELERY__RESULT_BACKEND: ${AIRFLOW__CELERY__RESULT_BACKEND}
    AIRFLOW__CORE__LOAD_EXAMPLES: ${AIRFLOW__CORE__LOAD_EXAMPLES}
    _PIP_ADDITIONAL_REQUIREMENTS: ${AIRFLOW_PIP_ADDITIONAL_REQUIREMENTS}
  volumes:
    - ../dags:/opt/airflow/dags
    - ../logs/airflow:/opt/airflow/logs
    - ../plugins:/opt/airflow/plugins
    - ./airflow/auth:/opt/airflow/auth
  restart: unless-stopped
  depends_on:
    airflow-postgres:
      condition: service_healthy
    airflow-redis:
      condition: service_healthy


services:
  # -----------------------------
  # ClickHouse (warehouse)
  # -----------------------------
  clickhouse:
    image: clickhouse/clickhouse-server:25.3-alpine
    container_name: dq_clickhouse
    env_file:
      - ./.env
    environment:
      CLICKHOUSE_DB: ${CLICKHOUSE_DB}
      CLICKHOUSE_USER: ${CLICKHOUSE_USER}
      CLICKHOUSE_PASSWORD: ${CLICKHOUSE_PASSWORD}
      CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT: ${CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT}
    ports:
      - "${CH_HTTP_PORT}:8123"
      - "${CH_NATIVE_PORT}:9000"
    ulimits:
      nofile:
        soft: 262144
        hard: 262144
    volumes:
      - ch_data:/var/lib/clickhouse
      - ch_logs:/var/log/clickhouse-server
      - ./init:/docker-entrypoint-initdb.d:ro
    healthcheck:
      test: ["CMD-SHELL", "clickhouse-client --query 'SELECT 1' >/dev/null 2>&1"]
      interval: 5s
      timeout: 5s
      retries: 30
    restart: unless-stopped

  ch-ui:
    image: ghcr.io/caioricciuti/ch-ui:latest
    container_name: dq_ch_ui
    env_file:
      - ./.env
    environment:
      CLICKHOUSE_URL: ${CHUI_CLICKHOUSE_URL}
      CONNECTION_NAME: ${CHUI_CONNECTION_NAME}
    ports:
      - "${CH_UI_PORT}:3488"
    volumes:
      - ch_ui_data:/app/data
    depends_on:
      clickhouse:
        condition: service_healthy
    restart: unless-stopped


  # -----------------------------
  # S3 On-Prem (SeaweedFS)
  # -----------------------------
  seaweed-master:
    image: chrislusf/seaweedfs:4.13
    container_name: dq_seaweed_master
    command: ["master","-ip=seaweed-master","-ip.bind=0.0.0.0","-port=9333","-mdir=/data"]
    ports:
      - "${SEAWEED_MASTER_PORT}:9333"
    volumes:
      - sw_master:/data
    restart: unless-stopped

  seaweed-volume:
    image: chrislusf/seaweedfs:4.13
    container_name: dq_seaweed_volume
    command: ["volume","-ip=seaweed-volume","-ip.bind=0.0.0.0","-port=8080","-dir=/data","-max=0","-mserver=seaweed-master:9333"]
    depends_on:
      - seaweed-master
    volumes:
      - sw_volume:/data
    restart: unless-stopped

  seaweed-filer:
    image: chrislusf/seaweedfs:4.13
    container_name: dq_seaweed_filer
    command: ["filer","-ip=seaweed-filer","-ip.bind=0.0.0.0","-port=8888","-master=seaweed-master:9333","-defaultStoreDir=/data"]
    ports:
      - "${SEAWEED_FILER_PORT}:8888"
    depends_on:
      - seaweed-master
      - seaweed-volume
    volumes:
      - sw_filer:/data
    restart: unless-stopped

  seaweed-s3:
    image: chrislusf/seaweedfs:4.13
    container_name: dq_seaweed_s3
    command: ["s3","-filer=seaweed-filer:8888","-ip.bind=0.0.0.0","-port=8333"]
    ports:
      - "${SEAWEED_S3_PORT}:8333"
    depends_on:
      - seaweed-filer
    restart: unless-stopped

  s3-init:
    image: amazon/aws-cli:2.15.57
    container_name: dq_s3_init
    env_file:
      - ./.env
    depends_on:
      - seaweed-s3
    volumes:
      - ./s3/init_buckets.sh:/scripts/init_buckets.sh:ro
    entrypoint: ["/bin/sh", "-lc"]
    command: "/scripts/init_buckets.sh"
    restart: "no"


  # -----------------------------
  # Airflow
  # -----------------------------
  airflow-postgres:
    image: postgres:16-alpine
    container_name: dq_airflow_postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: postgres
    ports:
      - "${AIRFLOW_POSTGRES_HOST_PORT}:5432"
    volumes:
      - airflow_pgdata:/var/lib/postgresql/data
      - ./airflow/postgres-init:/docker-entrypoint-initdb.d:ro
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow -d postgres"]
      interval: 5s
      timeout: 5s
      retries: 30
    restart: unless-stopped

  airflow-redis:
    image: redis:7-alpine
    container_name: dq_airflow_redis
    ports:
      - "${REDIS_HOST_PORT}:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 5s
      retries: 30
    restart: unless-stopped

  airflow-init:
    <<: *airflow-common
    container_name: dq_airflow_init
    restart: "no" 
    entrypoint: ["/bin/bash", "-lc"]
    command:
      - |
        set -e
        mkdir -p /opt/airflow/auth
        python - <<'PY'
        import json, os, pathlib
        u = os.environ.get("AIRFLOW_WWW_USER_USERNAME")
        p = os.environ.get("AIRFLOW_WWW_USER_PASSWORD")
        path = os.environ.get("AIRFLOW__CORE__SIMPLE_AUTH_MANAGER_PASSWORDS_FILE")
        pathlib.Path(path).parent.mkdir(parents=True, exist_ok=True)
        pathlib.Path(path).write_text(json.dumps({u: p}))
        print(f"[init] wrote simple auth passwords file: {path} (user={u})")
        PY
        airflow db migrate

  airflow-api-server:
    <<: *airflow-common
    container_name: dq_airflow_api_server
    ports:
      - "${AIRFLOW_PORT}:8080"
    command: ["airflow", "api-server"]
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8080/api/v2/monitor/health || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 30
    depends_on:
      airflow-init:
        condition: service_completed_successfully

  airflow-scheduler:
    <<: *airflow-common
    container_name: dq_airflow_scheduler
    command: ["airflow", "scheduler"]
    depends_on:
      airflow-init:
        condition: service_completed_successfully

  airflow-triggerer:
    <<: *airflow-common
    container_name: dq_airflow_triggerer
    command: ["airflow", "triggerer"]
    depends_on:
      airflow-init:
        condition: service_completed_successfully

  airflow-worker:
    <<: *airflow-common
    container_name: dq_airflow_worker
    command: ["airflow", "celery", "worker"]
    volumes:
      - ../dags:/opt/airflow/dags
      - ../logs/airflow:/opt/airflow/logs
      - ../plugins:/opt/airflow/plugins
      - /var/run/docker.sock:/var/run/docker.sock
      - ../:/workspace:ro
    depends_on:
      airflow-init:
        condition: service_completed_successfully

  flower:
    <<: *airflow-common
    container_name: dq_airflow_flower
    command: ["airflow", "celery", "flower"]
    ports:
      - "${FLOWER_PORT}:5555"
    depends_on:
      airflow-init:
        condition: service_completed_successfully


  # -----------------------------
  # Streamlit
  # -----------------------------
  streamlit:
    image: python:3.11-slim
    container_name: dq_streamlit
    env_file:
      - ./.env
    working_dir: /app
    volumes:
      - ../:/app
    environment:
      CLICKHOUSE_HOST: clickhouse
      CLICKHOUSE_HTTP_PORT: "8123"
      CLICKHOUSE_USER: ${CLICKHOUSE_USER}
      CLICKHOUSE_PASSWORD: ${CLICKHOUSE_PASSWORD}
      S3_ENDPOINT_URL: ${S3_ENDPOINT_URL_INTERNAL}
      LANDING_BUCKET: ${LANDING_BUCKET}
      ARTIFACTS_BUCKET: ${ARTIFACTS_BUCKET}
      DQREPORTS_BUCKET: ${DQREPORTS_BUCKET}
      DQFAILURES_BUCKET: ${DQFAILURES_BUCKET}
      AUDIT_BUCKET: ${AUDIT_BUCKET}
    ports:
      - "${STREAMLIT_PORT}:8501"
    depends_on:
      clickhouse:
        condition: service_healthy
      seaweed-s3:
        condition: service_started
    command:
      - /bin/sh
      - -lc
      - |
        set -eux
        python -m pip install --upgrade pip
        python -m pip install --no-cache-dir -r infra/requirements.txt
        python -m streamlit run apps/streamlit/app.py --server.address 0.0.0.0 --server.port 8501
    restart: unless-stopped

  dq-runner:
    image: python:3.11-slim
    container_name: dq_runner
    env_file:
      - ./.env
    working_dir: /app
    volumes:
      - ../:/app
    environment:
      CLICKHOUSE_HOST: clickhouse
      CLICKHOUSE_HTTP_PORT: ${CLICKHOUSE_HTTP_PORT}
      CLICKHOUSE_USER: ${CLICKHOUSE_USER}
      CLICKHOUSE_PASSWORD: ${CLICKHOUSE_PASSWORD}
      S3_ENDPOINT_URL: ${S3_ENDPOINT_URL_INTERNAL}
      LANDING_BUCKET: ${LANDING_BUCKET}
      ARTIFACTS_BUCKET: ${ARTIFACTS_BUCKET}
      DQREPORTS_BUCKET: ${DQREPORTS_BUCKET}
      DQFAILURES_BUCKET: ${DQFAILURES_BUCKET}
      AUDIT_BUCKET: ${AUDIT_BUCKET}
    depends_on:
      clickhouse:
        condition: service_healthy
      seaweed-s3:
        condition: service_started
    command:
      - /bin/sh
      - -lc
      - |
        set -eux
        python -m pip install --upgrade pip
        python -m pip install --no-cache-dir -r infra/requirements.txt
        sleep infinity
    restart: unless-stopped

volumes:
  ch_data:
  ch_logs:
  ch_ui_data:
  sw_master:
  sw_volume:
  sw_filer:
  airflow_pgdata: